# -*- coding: utf-8 -*-
"""Another copy of ML-Assignment3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10DFfLRcXuoYBKPgw9212X_wj0Ua1LBfR
"""

import pandas as pd
import numpy as np

# Load the MNIST dataset from a CSV file
mnist_csv = "/content/drive/MyDrive/ML/data.csv"  # Replace with your CSV file path
mnist_data = pd.read_csv(mnist_csv)
print(mnist_data.describe())
print(mnist_data.shape)
# Separate features and labels
y = mnist_data.iloc[:, 0].values  # Assuming the first column is the label
X_flattened = mnist_data.iloc[:, 1:].values  # The rest of the columns are features

# Normalize the pixel values
X_flattened = X_flattened / 255.0

# Reshape the flattened images to 2D for visualization
X_original = X_flattened.reshape(-1, 28, 28)

# One-hot encode the labels
def one_hot_encode(labels, num_classes=10):
    return np.eye(num_classes)[labels]

y_encoded = one_hot_encode(y)

# X_flattened is your feature set for training and y_encoded are your one-hot encoded labels
# X_original is the dataset in its original 2D format for visualization

import matplotlib.pyplot as plt

def visualize_original_image(X_original, index):
    image = X_original[index]
    plt.figure(figsize=(1, 1))
    plt.imshow(image, cmap = 'gray')
    plt.show()


# Example usage
index_to_visualize = 500  # Replace with the index of the image you want to view
visualize_original_image(X_original, index_to_visualize)

train = pd.read_csv("/content/drive/MyDrive/ML/data.csv")
train = train.drop("label", axis = 1)
idx = 780
digit_data = train.iloc[idx]
digit_image = digit_data.values.reshape(28, 28)

plt.figure(figsize=(1, 1))
plt.imshow(digit_image, cmap = 'gray')
plt.show()

"""#Neural Network Architecture"""

import numpy as np

class NeuralNetwork:
    def __init__(self, learning_rate=0.1):
        # Initialize weights and biases
      np.random.seed(13)  # Ensure reproducibility
      self.learning_rate = learning_rate
      self.weights1 = np.random.randn(784, 128) / np.sqrt(784)
      self.bias1 = np.ones(128)
      self.weights2 = np.random.randn(128, 64) / np.sqrt(128)
      self.bias2 = np.ones(64)
      self.weights3 = np.random.randn(64, 32) / np.sqrt(64)
      self.bias3 = np.ones(32)
      self.weights4 = np.random.randn(32, 10) / np.sqrt(32)
      self.bias4 = np.ones(10)

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def softmax(self, x):
      e_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Add axis=1
      return e_x / e_x.sum(axis=1, keepdims=True)


    def forward_pass(self, x):
      z1 = np.dot(x, self.weights1) + self.bias1
      self.a1 = self.sigmoid(z1)

      z2 = np.dot(self.a1, self.weights2) + self.bias2  # Use self.a1 here
      self.a2 = self.sigmoid(z2)

      z3 = np.dot(self.a2, self.weights3) + self.bias3  # Use self.a2 here
      self.a3 = self.sigmoid(z3)

      z4 = np.dot(self.a3, self.weights4) + self.bias4
      output = self.softmax(z4)

      return output



    def backprop(self, x, y, output):
        d_loss_output = output - y  # derivative of loss with respect to output

        d_weights4 = np.dot(self.a3.T, d_loss_output)
        d_bias4 = np.sum(d_loss_output, axis=0)

        d_loss_a3 = np.dot(d_loss_output, self.weights4.T) * self.sigmoid_derivative(self.a3)
        d_weights3 = np.dot(self.a2.T, d_loss_a3)
        d_bias3 = np.sum(d_loss_a3, axis=0)

        d_loss_a2 = np.dot(d_loss_a3, self.weights3.T) * self.sigmoid_derivative(self.a2)
        d_weights2 = np.dot(self.a1.T, d_loss_a2)
        d_bias2 = np.sum(d_loss_a2, axis=0)

        d_loss_a1 = np.dot(d_loss_a2, self.weights2.T) * self.sigmoid_derivative(self.a1)
        d_weights1 = np.dot(x.T, d_loss_a1)
        d_bias1 = np.sum(d_loss_a1, axis=0)

        self.weights1 -= self.learning_rate * d_weights1
        self.bias1 -= self.learning_rate * d_bias1
        self.weights2 -= self.learning_rate * d_weights2
        self.bias2 -= self.learning_rate * d_bias2
        self.weights3 -= self.learning_rate * d_weights3
        self.bias3 -= self.learning_rate * d_bias3
        self.weights4 -= self.learning_rate * d_weights4
        self.bias4 -= self.learning_rate * d_bias4

    def sigmoid_derivative(self, x):
        return x * (1 - x)
    def cross_entropy_loss(self, y_pred, y_true):
            # Small constant to avoid division by zero
            epsilon = 1e-15
            y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)
            return -np.mean(np.sum(y_true * np.log(y_pred_clipped), axis=1))

    def train(self, X_train, y_train, epochs, batch_size=23):
        history = {'loss': [], 'accuracy': []}
        n_samples = len(X_train)

        for epoch in range(epochs):
            indices = np.arange(n_samples)
            np.random.shuffle(indices)
            X_train_shuffled = X_train[indices]
            y_train_shuffled = y_train[indices]

            for start_idx in range(0, n_samples, batch_size):
                end_idx = min(start_idx + batch_size, n_samples)
                x_batch = X_train_shuffled[start_idx:end_idx]
                y_batch = y_train_shuffled[start_idx:end_idx]

                output = self.forward_pass(x_batch)
                self.backprop(x_batch, y_batch, output)

            predictions = self.forward_pass(X_train)
            epoch_loss = self.cross_entropy_loss(predictions, y_train)
            correct_predictions = np.sum(np.argmax(y_train, axis=1) == np.argmax(predictions, axis=1))
            epoch_accuracy = correct_predictions / n_samples

            history['loss'].append(epoch_loss)
            history['accuracy'].append(epoch_accuracy)

            print(f"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}")

        plt.figure(figsize=(12, 5))
        plt.subplot(1, 2, 1)
        plt.plot(history['loss'], label='Loss')
        plt.title('Loss over Epochs')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')

        plt.subplot(1, 2, 2)
        plt.plot(history['accuracy'], label='Accuracy')
        plt.title('Accuracy over Epochs')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()

        plt.show()

        return history

from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

def evaluate_model(model, X_test, y_test):
    predictions = model.forward_pass(X_test)
    y_pred = np.argmax(predictions, axis=1)
    y_true = np.argmax(y_test, axis=1)

    acc = accuracy_score(y_true, y_pred)
    cm = confusion_matrix(y_true, y_pred)

    print("Accuracy:", acc)
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt='g')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

"""#For the 70:30 Split

"""

X_train_70, X_test_30, y_train_70, y_test_30 = train_test_split(X_flattened, y_encoded, test_size=0.3, random_state=42)
nn_70 = NeuralNetwork(learning_rate=0.1)
print("Training with 70:30 train-test split")
nn_70.train(X_train_70, y_train_70, epochs=25, batch_size=23)
evaluate_model(nn_70, X_test_30, y_test_30)

"""# For the 80:20 Split

"""

X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X_flattened, y_encoded, test_size=0.2, random_state=42)

nn_80 = NeuralNetwork(learning_rate=0.1)
print("Training with 80:20 train-test split")
nn_80.train(X_train_80, y_train_80, epochs=25, batch_size=23)
evaluate_model(nn_80, X_test_20, y_test_20)

"""# Training with 90:10 Split

"""

X_train_90, X_test_10, y_train_90, y_test_10 = train_test_split(X_flattened, y_encoded, test_size=0.1, random_state=42)

nn_90 = NeuralNetwork(learning_rate=0.1)
print("Training with 90:10 train-test split")
nn_90.train(X_train_90, y_train_90, epochs=25, batch_size=23)
evaluate_model(nn_90, X_test_10, y_test_10)